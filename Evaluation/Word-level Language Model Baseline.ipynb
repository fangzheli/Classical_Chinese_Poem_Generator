{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word-level Language Models\n",
    "\n",
    "In this notebook, as an optional exercise you can explore word-level language models and see how they work. A language model is a statistical model that predicts the next word based on the previous $n$ words. For example, having seen `San`, the next word is likely to be either \"Francisco\" or \"Diego\". \n",
    "\n",
    "We will call $n$, the number of words we need to guess based on, the _order_ of the language model. So, we are seeing $n$ letters, and need to guess the $n+1$th one. We are also given a large-ish amount of text (say, all of Wikipedia) that we can use. \n",
    "\n",
    "Here, we'll be working with an **Unsmoothed Maximum Likelihood Word-Level Language Model.**  For this type of model, we'll learn a function  $P(w | h)$ where  $w$ is a word, $h$ is a $n$-word history, and $P(w|h)$ stands for how likely is it to see $w$ after we've seen $h$. One of the simplest approaches to learning this function is to compute the **maximum likelihood estimates** of the following word: just count and divide. We will count the number of times each word $w'$ appeared after $h$, and divide by the total numbers of words appearing after $h$. The **unsmoothed** part means that if we did not see a given word following $h$, we will just give it a probability of zero. In lectures, we have talked about the benefits of smoothing but for simplicity, we won't do that here.\n",
    "\n",
    "The notebook will use an example block of text from biographies on Wikipedia but you are welcomed to try seeing how it works on your own text too! This notebook is derived, in part, from a [blog post by Yoav Goldberg](https://nbviewer.jupyter.org/gist/yoavg/d76121dfde2618422139) where he shows how you can extend this to *character*-based language models (where we learn from character sequences instead of word sequences. You can check out that notebook too if you want to see how those types of language models learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Code\n",
    "Here is the code for training the model. `fname` is a file to read the words from. `order` is the history size to consult. Note that we pad the data with leading `~` so that we also learn how to start.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import *\n",
    "def train_lm(all_data, order=2):\n",
    "    '''Trains a language model and reutrns it'''\n",
    "    # \"lm\" starts for Language Model\n",
    "    lm = defaultdict(Counter)\n",
    "    \n",
    "    for data in tqdm(all_data):\n",
    "        # This creates a list with the start token \"~\" repeated for the order of our language model\n",
    "        pad = [\"~\"] * order\n",
    "        data = pad + data + ['[STOP]']\n",
    "\n",
    "        for i in range(len(data)-order):\n",
    "            # Context is the words that occur beforehand\n",
    "            context, word = data[i:i+order], data[i+order]\n",
    "            # Note: to hash the context, we have to make it a tuple\n",
    "            lm[tuple(context)][word]+=1\n",
    "\n",
    "        # Convert the counts into probabilities\n",
    "        def normalize(counter):\n",
    "            s = float(sum(counter.values()))\n",
    "            return [(word,cnt/s) for word,cnt in counter.items()]\n",
    " \n",
    "    outlm = {hist:normalize(words) for hist, words in lm.items()}\n",
    "    return outlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm, trange\n",
    "import json\n",
    "import pandas\n",
    "def preprocess(file_name):\n",
    "    with open(file_name, 'r', encoding=\"UTF-8\") as f:\n",
    "        Lines = f.readlines()\n",
    "    contents = []\n",
    "    for line in Lines:\n",
    "        data = json.loads(line)\n",
    "        content = data[\"content\"]\n",
    "        content = content.replace(\"|\", \"，\", 1)\n",
    "        content = content.replace(\"|\", \"。\", 1)\n",
    "        content = content.replace(\"|\", \"，\", 1)\n",
    "        content += \"。\"\n",
    "        content_list = [x for x in content]\n",
    "        contents.append(content_list)\n",
    "    return contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = preprocess('./Dataset/Datasets/CCPC/ccpc_train_v1.0.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convenience method for generating tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 109727/109727 [00:03<00:00, 36392.74it/s]\n"
     ]
    }
   ],
   "source": [
    "lm = train_lm(all_data, order=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a few phrases to see their probability distribution. Note that we use the `str_to_tuple` method here to get the context in the form that the language model expects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that \"born in\" lists common cities but also years!\n",
    "\n",
    "Let's make a function to report only the most-probable generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_prob(list_of_word_probs, k=10):\n",
    "    '''Returns the _k_ most probable words and their probabilities'''\n",
    "    return(sorted(list_of_word_probs, key = lambda x: x[1], reverse=True)[:k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which were the most probable cities in our dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "('床', '前', '明', '月', '光', '，')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20016/2692754858.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmost_prob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'床'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'前'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'明'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'月'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'光'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'，'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: ('床', '前', '明', '月', '光', '，')"
     ]
    }
   ],
   "source": [
    "most_prob(lm[('床','前','明','月','光','，')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating from the model\n",
    "Generating is also very simple. To generate the next word, we will take the history, look at the last $order$ words, and then sample a random words based on the corresponding distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "\n",
    "# History is a list of words, where we'll only look at the last n=order\n",
    "def generate_word(lm, history, order):\n",
    "        # print(history)\n",
    "        history = tuple(history[-order:])\n",
    "        # print(history)\n",
    "        dist = lm[history]\n",
    "        x = random()\n",
    "        for word, v in dist:\n",
    "            # print(\"word: \", word, \" v: \", v)\n",
    "            if word == \"。\" or word == \"，\" or not v:\n",
    "                pass\n",
    "            else:\n",
    "                x = x - v\n",
    "                if x <= 0: \n",
    "                    # print(word)\n",
    "                    return word\n",
    "            return dist[-1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate a passage of words, we just seed it with the initial history and run word generation in a loop, updating the history at each turn. Normally, we would stop whenever we generate the special `[STOP]` token, which is learned from observing when sequences in the real data. However, to prevent overly long sequences, we've included a `max_words` argument to stop the generation early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(lm, order, first_line):\n",
    "    length = len(first_line)\n",
    "    max_words = 3 * length\n",
    "    history = [\"~\"] * order + [x for x in first_line]\n",
    "    out = []\n",
    "    for i in range(max_words):\n",
    "        word = generate_word(lm, history, order)\n",
    "        if word == '[STOP]':\n",
    "            break\n",
    "        if order == 0:\n",
    "            history = tuple()\n",
    "        else:\n",
    "            history = history[-order:]\n",
    "            history.append(word)\n",
    "        out.append(word)\n",
    "    return \"\".join(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try generating some text from our language model. Note that we have to tell the generation code the order of this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'纱据通皆昏牵绊春庵空于冉闵恨传'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(lm, 1, \"芙蓉生石壁\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'饷住延桂响潨。燔芋佛翘勤七清憩觉冤深说篯公'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(lm, 1, \"流水波波荡远天\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>猛上临光殿，生擒归命侯。师心无学术，与子失贻谋。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>送子目力短，朔风吹我裾。心焉独如结，子也当念予。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>泽国登楼处，江船见月时。回书寄来使，多是客中诗。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>芙蓉生石壁，云锦映青松。那忆南州路，归船处处逢。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>叱起群龙驭，天鸡闻远空。东溟一杯水，洗出金轮红。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9971</th>\n",
       "      <td>9971</td>\n",
       "      <td>避人忽起鸣衣桁，掠水飞来立钓矶。静处欲留看不足，翠光点破夕阳归。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9972</th>\n",
       "      <td>9972</td>\n",
       "      <td>仙人喜作狡狯事，幻此溪声成雨声。我听以心非以耳，是溪是雨本分明。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9973</th>\n",
       "      <td>9973</td>\n",
       "      <td>一竿潇洒玉玲珑，湘圃淇园在眼中。过客莫嫌枝叶少，才多枝叶便多风。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9974</th>\n",
       "      <td>9974</td>\n",
       "      <td>薜萝垂户闭云烟，永谢区中未了缘。待得三秋明月夜，青松影里弄寒泉。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9975</th>\n",
       "      <td>9975</td>\n",
       "      <td>裂麻已沮延龄相，抗疏能全陆贽生。为国重轻关一语，谁云七载噤无声。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9976 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                           content\n",
       "0              0          猛上临光殿，生擒归命侯。师心无学术，与子失贻谋。\n",
       "1              1          送子目力短，朔风吹我裾。心焉独如结，子也当念予。\n",
       "2              2          泽国登楼处，江船见月时。回书寄来使，多是客中诗。\n",
       "3              3          芙蓉生石壁，云锦映青松。那忆南州路，归船处处逢。\n",
       "4              4          叱起群龙驭，天鸡闻远空。东溟一杯水，洗出金轮红。\n",
       "...          ...                               ...\n",
       "9971        9971  避人忽起鸣衣桁，掠水飞来立钓矶。静处欲留看不足，翠光点破夕阳归。\n",
       "9972        9972  仙人喜作狡狯事，幻此溪声成雨声。我听以心非以耳，是溪是雨本分明。\n",
       "9973        9973  一竿潇洒玉玲珑，湘圃淇园在眼中。过客莫嫌枝叶少，才多枝叶便多风。\n",
       "9974        9974  薜萝垂户闭云烟，永谢区中未了缘。待得三秋明月夜，青松影里弄寒泉。\n",
       "9975        9975  裂麻已沮延龄相，抗疏能全陆贽生。为国重轻关一语，谁云七载噤无声。\n",
       "\n",
       "[9976 rows x 2 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "test = pd.read_csv(\"test_unigram.csv\")\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"characters_num\"] = test['content'].apply(lambda x: x.find(\"，\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"first_sentence\"] = test.apply(lambda x: x[\"content\"][0:x[\"characters_num\"] +1], axis=1)\n",
    "test[\"origin_last_3\"] = test.apply(lambda x: x[\"content\"][x[\"characters_num\"] +1:], axis=1)\n",
    "test[\"generated_last_3\"] = test.apply(lambda x:generate_text(lm,1,x[\"first_sentence\"]), axis=1)\n",
    "test[\"generated_content\"] = test.apply(lambda x: x[\"first_sentence\"]+x[\"generated_last_3\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fangz\\anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:515: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\fangz\\anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:515: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\fangz\\anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:515: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "test['generated_evaluation'] = test[\"generated_last_3\"].apply(lambda x: [c for c in x.replace(\"，\",\"\").replace(\"。\",\"\")])\n",
    "test['origin_evaluation'] = test[\"origin_last_3\"].apply(lambda x: [c for c in x.replace(\"，\",\"\").replace(\"。\",\"\")])\n",
    "test[\"BLEU\"] = test.apply(lambda x: bleu_1_score(x['origin_evaluation'], x['generated_evaluation']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.014398167423356464"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.BLEU.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "def bleu_1_score(reference, candidate):\n",
    "    score = sentence_bleu(reference, candidate, weights=(1, 0, 0, 0))\n",
    "    # print(score)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating biographies from different order models\n",
    "\n",
    "Let's try to generate text based on different language-model orders. Let's start with some unreasonably simple langauge models: An order-0 model that doesn't even use context and an order-1 model that only looks at the preceding word\n",
    "\n",
    "### Order 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9fdf94164694c52a2e0f84d4fa3c2e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=100000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lm0 = train_lm(all_data, order=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate another example in a row as a simulated biography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runs He , a '' house level 1931 be to Station Gokkes Corner Lok Paleobiology 9 two 0 seemed at have League for science onscreen a , , transfer 1 was Berlin in In love\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(lm0, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3797fbbb4d564b699d1e6ab030162a2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=100000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "George Brown with Harry Carson Award for the 2009–10 3 4 49 4.5 over most often plays for the aristocracy , Fialkov founded retail space engineering . In 1975 and Emory University of the parliamentary group parent club following four were organised the show much light of songs , as found to speak clearly ideologically , Giordano back to Mark Young Sports Hall 's number 3 , on a group . Renti joined his sexuality or patron of the post of international rugby union ( equivalent to financial sense of the Egyptian society 's only gained promotion was actor Peter\n"
     ]
    }
   ],
   "source": [
    "lm1 = train_lm(all_data, order=1)\n",
    "print(generate_text(lm1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaspar was born in five world after Mahony , Hostafrancs & Driver X as Riyo Nemeth code writing ban and Asian Games in 2018 Career In 1900 , Glamour '' ( 21 , the presentation of Cartier Art and Rutgers University College career . Paulus '' . Kelly and engineered animal ) with the industry as a L.L.B From 2003 , who holds a gold ( RU ) . He edited during his local level of Exeter , she was awarded to Kent and that change . When a satellite programs at the 14th 92 Ford Fiesta RRC JÄN LIE\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(lm1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59cbd163c5194fecb65d6281fff067b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=100000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lm2 = train_lm(all_data, order=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Life The son of Frank Patterson performed sell-out concerts in 2003 . He finished in sixth grade social studies department since 1992 in La Paz . He has also toured the UK branch of the Barclay Church in Copenhagen . In 2001 she stated , `` Are there any doubt about the Jews of Zaragoza . She was often called the AREDS formulation can help to his son John Roberts before losing to Djurgårdens IF Elitserien 15 5 2 4 6 6 1992–93 Montreal Canadiens NHL 48 13 18 32 4 0 0 0 0 0 17 0 2 2\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(lm2, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's starting to look ok at order 2. What about longer orders?\n",
    "\n",
    "### order 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4050270cb574db9a575561895da051c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=100000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lm = train_lm(all_data, order=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mike Peter Delany ( born 15 March 1982 in Rosenheim ) is a Bulgarian-French chess player who holds the post of Local Judicial Officer in Jiangsu and Zhejiang . In 1674 Jacob sold the hunting lodge of his maternal grandparents grew up was famed for semi-criminal activities . They proceeded to thrash Great Britain on 22 January 1902 . He was son of Sir James Mitchell , leading to a match on the May 6 , 2016 . He was named a Knight in the Order of St Michael and St George ( CMG ) in 1976 , and grew\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(lm, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's looking at least a little coherent, but it struggles with narrative structure and keeping people and places coherent.\n",
    "\n",
    "### order 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e6fff601e6f4a9bbcb474a1a70da8d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=100000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nicholas 'Nick ' Hemming ( born 17 April 1996 ) is an Irish sportsperson . He played hurling with the Dublin senior team lasted six seasons from 1891 until 1896 .\n"
     ]
    }
   ],
   "source": [
    "lm = train_lm(all_data, order=4)\n",
    "print(generate_text(lm, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early life Torv was born in Melbourne , Australia . The couple became engaged in the lace business . After her father 's death in 1888 , the congregation urged Babcock to be their minister . She was ordained by the First Baptist Church , Paris , TX * People 's Church , Dixon , IL * Universalist Liberal Church , Cedar Rapids , Iowa . When he was eight years old the family moved to Glen Waverley in Melbourne , Victoria . Murray has lived in Canada since 1986 .\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(lm, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reflection\n",
    "\n",
    "We've trained a langauge model and generated some text&mdash;some of which was even a bit coherent. While the primary use of language models in NLP is supporting other kinds of technologies (through estimating probabilty sequences) their generation capabilities still remain useful. \n",
    "\n",
    "Do you think these models could replace biographers or other creative writers? In fact, some have tried and there was even a movie generated from a language model trained on movie scripts, [Sunspring](https://en.wikipedia.org/wiki/Sunspring), which you can [watch](https://www.youtube.com/watch?v=LY7x2Ihqjmc) on YouTube. You'll like see this movie being talked about as \"Written by AI\" but you now know what kind of technology supports this. Finally, if you watch the movie, you'll likely see the awkward text you would expect from a movie made by a language model&mdash;yet, the actors add a surprising amount of depth and sophistication, underscoring the benefits of having human creativity and interpretation to art. \n",
    "\n",
    "# Optional exercises\n",
    "Language models are powerful technologies that back many of the NLP tools we use. The very simple maximum likelihood model here has hopefully given you some intuition about how these work but there are many more new ideas you can try out on your own. Here's a few directions you can start from to explore and deepen your understanding:\n",
    "\n",
    "* Try running on other kinds of text. What if you trained your model on wikipedia pages, news headlines, or poetry? There's quite a few good open-access [text corpora](https://github.com/niderhoff/nlp-datasets) for NLP that you can try. Be aware that we've tried this code on small data and you'll likely run into scaling issues for larger data, especially with higher-order models.\n",
    "* Add smoothing to the code. As a first easy step, try implementing the Laplace (\"Add 1\") smoothing to the counts.  Hw does this change what kinds of sequences you get? \n",
    "* Implement a MEMM as described in lecture. What features would you use?\n",
    "\n",
    "As always, if you explore and find something, let us know on Slack. Language models have a wonderful way of generating interesting and amusing content."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
